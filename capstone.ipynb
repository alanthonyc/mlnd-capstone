{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Udacity Machine Learning Nanodegree\n",
    "\n",
    "## Capstone Project — Tweet Classifier\n",
    "\n",
    "> *Classify tweets as either __Republican__ or __Democrat__.*\n",
    "\n",
    "The goal of this project is to create a neural network text classifier that performs better then a benchmark Naive Bayes classifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Examine the Dataset\n",
    "\n",
    "Some summary stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 86460\n",
      "------\n",
      "Tweets by party:\n",
      "Party\n",
      "Democrat      42068\n",
      "Republican    44392\n",
      "Name: Tweet, dtype: int64\n",
      "------\n",
      "Mean number tweets per account: 199.676674\n",
      "Median number of tweets per account: 200.000000\n",
      "Number of accounts without exactly two hundred tweets: 17.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tweets = pd.read_csv('./dataset/ExtractedTweets.csv')\n",
    "#tweets.head()\n",
    "print \"Total number of tweets: %i\" % tweets['Tweet'].count()\n",
    "\n",
    "print \"------\"\n",
    "party_count = tweets.groupby(['Party',]).count()\n",
    "print (\"Tweets by party:\")\n",
    "print(party_count['Tweet'])\n",
    "\n",
    "print \"------\"\n",
    "handles = None\n",
    "handle_counts = None\n",
    "handles = tweets.groupby('Handle')\n",
    "handle_counts = handles.count()\n",
    "non_two_hundred = handle_counts[handle_counts['Tweet']!=200]['Tweet'].count()\n",
    "print \"Mean number tweets per account: %f\" % handle_counts['Tweet'].mean()\n",
    "print \"Median number of tweets per account: %f\" % handle_counts['Tweet'].median()\n",
    "print \"Number of accounts without exactly two hundred tweets: %f\" % non_two_hundred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an almost perfectly balanced binary-labeled dataset. There are a total of **86460** tweets.  **42068** (48.65%) are labeled *Democrat*, while **44392** (51.34%) are labeled *Republican*. All but seventeen (17) of the accounts included in this data set made exactly two-hundred (200) tweets each.\n",
    "\n",
    "This data set is ideal for doing a binary classification based solely on the content of the text in the dataset, with minimal influence from data composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Vectorize\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts = None\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Today, Senate Dems vote to #SaveTheInternet. P...\n",
      "1    RT @WinterHavenSun: Winter Haven resident / Al...\n",
      "2    RT @NBCLatino: .@RepDarrenSoto noted that Hurr...\n",
      "3    RT @NALCABPolicy: Meeting with @RepDarrenSoto ...\n",
      "4    RT @Vegalteno: Hurricane season starts on June...\n",
      "5    RT @EmgageActionFL: Thank you to all who came ...\n",
      "6    Hurricane Maria left approx $90 billion in dam...\n",
      "7    RT @Tharryry: I am delighted that @RepDarrenSo...\n",
      "8    RT @HispanicCaucus: Trump's anti-immigrant pol...\n",
      "9    RT @RepStephMurphy: Great joining @WeAreUnidos...\n",
      "Name: Tweet, dtype: object\n",
      "RangeIndex(start=0, stop=10, step=1)\n"
     ]
    }
   ],
   "source": [
    "#X_train_counts = vectorizer.transform(tweets['Tweet'])\n",
    "#X_train_counts.shape\n",
    "\n",
    "tweet_sample = tweets['Tweet'][0:10]\n",
    "print tweet_sample\n",
    "print tweet_sample.index\n",
    "\n",
    "v = CountVectorizer()\n",
    "v.fit(tweet_sample)\n",
    "\n",
    "#print(v.vocabulary_)\n",
    "#v.vocabulary_\n",
    "\n",
    "t = v.transform(tweets['Tweet'][2:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126329\n",
      "index of 'senate': 98443\n",
      "[ 7.15796257  5.59322484 11.67430154 ... 11.67430154 11.67430154\n",
      " 11.26883644]\n",
      "tfidf of 'senate': 5.598956\n",
      "tweet:\n",
      "  (0, 74945)\t0.4856049705506262\n",
      "  (0, 28191)\t0.052833650863872844\n",
      "  (0, 53345)\t0.05292198936794678\n",
      "  (0, 52688)\t0.17166959365086887\n",
      "  (0, 105638)\t0.06484465864207267\n",
      "  (0, 55875)\t0.09591415175080434\n",
      "  (0, 51255)\t0.1902203520427344\n",
      "  (0, 66211)\t0.22046448919469594\n",
      "  (0, 76460)\t0.2783065606436417\n",
      "  (0, 99761)\t0.3525527653079148\n",
      "  (0, 103134)\t0.1957869759852857\n",
      "  (0, 87421)\t0.19415404163917044\n",
      "  (0, 97338)\t0.3492886551285527\n",
      "  (0, 106897)\t0.13840569968604383\n",
      "  (0, 114602)\t0.21903540977450434\n",
      "  (0, 33246)\t0.3015487758119325\n",
      "  (0, 98443)\t0.23289450050801297\n",
      "  (0, 106944)\t0.13729330830673972\n",
      "106944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfv = TfidfVectorizer()\n",
    "doc = tfv.fit_transform(tweets['Tweet'])\n",
    "print(len(tfv.vocabulary_.keys()))\n",
    "index_of_senate = tfv.vocabulary_.get(u'senate')\n",
    "print \"index of 'senate': %i\" % index_of_senate\n",
    "print tfv.idf_\n",
    "print \"tfidf of 'senate': %f\" % tfv.idf_[index_of_senate]\n",
    "\n",
    "print \"tweet:\"\n",
    "print doc[0]\n",
    "\n",
    "print tfv.vocabulary_.get(u'today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
